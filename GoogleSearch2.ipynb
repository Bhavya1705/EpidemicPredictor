{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday's date: 2020-08-28\n",
      "https://www.livemint.com/mint-lounge/features/a-virtual-new-beginning-for-baro-11598677030010.html\n",
      "https://health.economictimes.indiatimes.com/news/industry/china-study-warns-of-possible-new-pandemic-virus-from-pigs/76721325\n",
      "https://www.bbc.com/news/world-51235105\n",
      "https://www.cdc.gov.tw/En\n",
      "https://www.cdc.gov.tw/En/Bulletin/Detail/pevkg6JalT40I4uB6wAWcg?typeid=158\n",
      "https://www.cdc.gov.tw/En/Bulletin/Detail/YS_6xZ01kBHnOpamUwyEgA?typeid=158\n",
      "https://www.cdc.gov.tw/En/Bulletin/List/7tUXjTBf6paRvrhEl-mrPg\n",
      "https://www.cdc.gov.tw/En/Category/Page/yZOu-4cGeu77HDyzE0ojqg\n",
      "http://outbreaknewstoday.com/\n",
      "https://www.bbc.com/news/world-51235105\n",
      "https://apnews.com/VirusOutbreak\n",
      "https://indianexpress.com/article/india/india-unlock-4-0-guidelines-6575245/\n",
      "https://www.india.com/news/india/lockdown-in-uttar-pradesh-religious-social-programmes-restricted-in-state-till-sept-30-weekend-shutdown-to-be-strictly-implemented-4125020/\n",
      "https://economictimes.indiatimes.com/topic/new-lockdown-guidelines\n",
      "https://economictimes.indiatimes.com/topic/lockdown\n",
      "https://www.newindianexpress.com/states/kerala/2020/aug/29/smell-the-coffee-test-for-kerala-officials-to-skip-quarantine-after-travel-2189717.html\n",
      "https://www.bbc.com/news/uk-53937997\n",
      "https://economictimes.indiatimes.com/topic/quarantine\n",
      "https://www.bbc.co.uk/news/explainers-52544307\n",
      "https://apnews.com/VirusOutbreak\n",
      "https://apnews.com/UnderstandingtheOutbreak\n",
      "https://apnews.com/33ce920720ee71dbabcd8db757b8dfd3\n",
      "https://apnews.com/OneGoodThing\n",
      "https://health.economictimes.indiatimes.com/news/industry/china-study-warns-of-possible-new-pandemic-virus-from-pigs/76721325\n"
     ]
    }
   ],
   "source": [
    "from datetime import date,timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import geograpy \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import openpyxl\n",
    "import html2text\n",
    "import pycountry\n",
    "import spacy\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pt_core_news_sm\n",
    "file=openpyxl.Workbook()\n",
    "std=file['Sheet']\n",
    "std.append(['url','flag','Country','News'])\n",
    "h = html2text.HTML2Text()\n",
    "# Ignore converting links from HTML\n",
    "h.ignore_links = True\n",
    "today = date.today()\n",
    "yesterday = today - timedelta(days=1)\n",
    "print(\"Yesterday's date:\", yesterday)\n",
    "try: \n",
    "    from googlesearch import search \n",
    "except ImportError:  \n",
    "    print(\"No module named 'google' found\") \n",
    "keywords=['new pandemic','new epidemic','new outbreak','new lockdown','new quarantine','deaths','new viral outbreak']\n",
    "article=[]\n",
    "for query in keywords:  \n",
    "    for j in search(query+\" after:\"+str(yesterday), tld=\"com\", num=100, stop=5, pause=2): \n",
    "        if j.find('corona')==-1 and j.find('covid')==-1:\n",
    "            print(j)\n",
    "            article.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rishCoronavirus\n",
      "'pandemic virus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Coronavirus\n",
      " of Enterovirus\n",
      "vel coronavirus\n",
      " of Enterovirus\n",
      " of Enterovirus\n",
      " of Enterovirus\n",
      "West Nile virus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Coronavirus\n",
      "worldwide virus\n",
      "the coronavirus\n",
      "ews\n",
      "Coronavirus\n",
      "vers from virus\n",
      "vers from virus\n",
      "\n",
      "Suka\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Coronavirus\n",
      "vers from virus\n",
      "worldwide virus\n",
      "new coronavirus\n",
      "spread of virus\n",
      "'pandemic virus\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "raspin=1\n",
    "for it in article:\n",
    "    try:\n",
    "        #print(i)\n",
    "        flag=0\n",
    "        url = it#\"https://health.economictimes.indiatimes.com/news/industry/china-study-warns-of-possible-new-pandemic-virus-from-pigs/76721325\"\n",
    "        places = geograpy.get_place_context(url=url) \n",
    "        Location=[]\n",
    "        page=requests.get(url)\n",
    "        soup = BeautifulSoup.get_text(BeautifulSoup(page.text, 'html.parser'))\n",
    "        viral_data=soup.find(\"virus\")\n",
    "        virus=soup[viral_data-10:viral_data+5]\n",
    "        #print(virus)\n",
    "        places = geograpy.get_place_context(url=url) \n",
    "        max=0\n",
    "        for i in places.country_mentions:\n",
    "            try:\n",
    "                if pycountry.countries.lookup(i[0]):\n",
    "                    Location.append(pycountry.countries.lookup(i[0]).name)\n",
    "                    if max< i[1]:\n",
    "                        max=i[1]\n",
    "                        max_location=i[0]\n",
    "            except:\n",
    "                pass\n",
    "        #nlp = pt_core_news_sm.load()\n",
    "        if soup.find('Denied')!=-1 or soup.find('connect to the server')!=-1 :\n",
    "            print(\"Suka\")\n",
    "            continue\n",
    "        nlp=spacy.load('en_core_web_sm')#en_core_web_sm.load()\n",
    "        f=soup\n",
    "        text=f\n",
    "        #text = f.replace(\"  \",\"\")\n",
    "        #text=f.replace(\"\\n\",\" \")\n",
    "        doc = nlp(text)\n",
    "        entities=[]\n",
    "        something=[]\n",
    "        something_significant=\"\"\n",
    "        matches = [\"affect\", \"dea\", \"case\",\"patients\"]\n",
    "        for i in doc.ents:\n",
    "            if i.label_=='CARDINAL':\n",
    "                entities.append((i.text,text[i.start_char-10:i.end_char+10], i.label_))#, i.label))\n",
    "                something.append(text[i.start_char-30:i.end_char+30])\n",
    "                if any(x in text[i.start_char:i.end_char+20] for x in matches):\n",
    "                    #print(text[i.start_char:i.end_char+20])\n",
    "                    #print(text[i.start_char:i.end_char+20].find(\"dea\"))\n",
    "                    #print(matches)\n",
    "                    if text[i.start_char:i.end_char+20].find(\"affect\")!=-1:\n",
    "                        matches.remove(\"affect\")\n",
    "                    if text[i.start_char:i.end_char+20].find(\"dea\")!=-1:\n",
    "                        matches.remove(\"dea\")\n",
    "                    if text[i.start_char:i.end_char+20].find(\"case\")!=-1:\n",
    "                        matches.remove(\"case\")\n",
    "                    if text[i.start_char:i.end_char+20].find(\"patients\")!=-1:\n",
    "                        matches.remove(\"patients\")\n",
    "                    if matches==[]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pass\n",
    "                    '''\n",
    "                    for num in range(i.start_char,i.start_char-100,-1):\n",
    "                        if text[i.start_char-num]==\".\" or text[i.start_char-num-1:i.start_char-num]==\"  \" :\n",
    "                            break\n",
    "                    for num2 in range(i.end_char,i.end_char+100,1):\n",
    "                        try:\n",
    "                            if text[i.end_char+num2]==\".\":\n",
    "                                break\n",
    "                        except:\n",
    "                            break\n",
    "                    '''\n",
    "                    something_significant=something_significant+text[i.start_char:i.end_char+35]+\". \"\n",
    "                    #something_significant.append(text[i.start_char:i.end_char+100])\n",
    "            else: \n",
    "                pass \n",
    "        corpus = [sent.text.lower() for sent in doc.sents ]\n",
    "        cv = CountVectorizer(stop_words=list(STOP_WORDS))   \n",
    "        cv_fit=cv.fit_transform(corpus)    \n",
    "        word_list = cv.get_feature_names();    \n",
    "        count_list = cv_fit.toarray().sum(axis=0)\n",
    "        word_frequency = dict(zip(word_list,count_list))\n",
    "        val=sorted(word_frequency.values())\n",
    "        higher_word_frequencies = [word for word,freq in word_frequency.items() if freq in val[-3:]]\n",
    "        #print(\"\\nWords with higher frequencies: \", higher_word_frequencies)# gets relative frequency of words\n",
    "        higher_frequency = val[-1]\n",
    "        for word in word_frequency.keys():  \n",
    "            word_frequency[word] = (word_frequency[word]/higher_frequency)\n",
    "        sentence_rank={}\n",
    "        for sent in doc.sents:\n",
    "            for word in sent :       \n",
    "                if word.text.lower() in word_frequency.keys():            \n",
    "                    if sent in sentence_rank.keys():\n",
    "                        sentence_rank[sent]+=word_frequency[word.text.lower()]\n",
    "                    else:\n",
    "                        sentence_rank[sent]=word_frequency[word.text.lower()]\n",
    "        top_sentences=(sorted(sentence_rank.values())[::-1])\n",
    "        top_sent=top_sentences[:3]\n",
    "        summary=[]\n",
    "        for sent,strength in sentence_rank.items():  \n",
    "            if strength in top_sent:\n",
    "                summary.append(sent)\n",
    "            else:\n",
    "                continue \n",
    "        #print(url,max_location,something_significant,sep=\" \",end=\"\\n################3\\n\")\n",
    "        summary=\"\"\n",
    "        for i in summary:\n",
    "            print(i,end=\" \")\n",
    "            summary=summary+\" \"+i\n",
    "        #print(\"\\nLOCATION####################\\n\")\n",
    "        #print(Location,end=\"\\nSomething####################\\n\")\n",
    "        #print(something,end=\"\\nEntities####################\\n\")\n",
    "        #print(entities,end=\"\\n###########################################################################3\\n \")\n",
    "        #strings=strings+\"\\n####################\\n\"+str(Location)+\"\\n####################\\n\"+str(entities)+\"\\n############\"+str(something)+\"###############################################################3\\n \"\n",
    "        strings=url+\"\\n\"+str(max_location)+\"\\n\"+str(something_significant)+\"\\n##########\\n\"\n",
    "        if len(something_significant)>1 and len(virus)!=0:\n",
    "            with open(\"ReportFile.txt\", \"a\") as file1: \n",
    "                file1.write(strings)\n",
    "                std.append([url,1,str(max_location),str(max_location)+\" \"+virus+\" \"+str(something_significant)+\" \"+url,summary])\n",
    "                raspin+=1\n",
    "    except:\n",
    "        pass\n",
    "file.save(\"ReportFile.xlsx\")            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
